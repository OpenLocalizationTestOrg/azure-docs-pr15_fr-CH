<properties
   pageTitle="Liste de vérification de l’évolutivité | Microsoft Azure"
   description="Conseils de liste de contrôle de l’évolutivité pour les problèmes de conception pour Azure Autoscaling."
   services=""
   documentationCenter="na"
   authors="dragon119"
   manager="christb"
   editor=""
   tags=""/>

<tags
   ms.service="best-practice"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="07/13/2016"
   ms.author="masashin"/>

# <a name="scalability-checklist"></a>Liste de vérification de l’évolutivité

[AZURE.INCLUDE [pnp-header](../includes/guidance-pnp-header-include.md)]

## <a name="service-design"></a>Conception du service
- **Partition de la charge de travail**. Concevoir des composants du processus discrets et DÉCOMPOSABLES. Réduire la taille de chaque partie, tout en respectant les règles habituelles pour la séparation des préoccupations et le principe de responsabilité unique. Ainsi, les composants doit être distribué d’une manière qui optimise l’utilisation de chaque unité de calcul (par exemple, un serveur de base de données ou rôle). Il facilite également à l’échelle de l’application en ajoutant des instances de ressources spécifiques. Pour plus d’informations, voir [calculer le partitionnement](https://msdn.microsoft.com/library/dn589773.aspx).
- **Conception de la mise à l’échelle**. Mise à l’échelle permet aux applications de réagir à charge variable en augmentant et diminution du nombre d’instances de rôles, des files d’attente et autres services qu’ils utilisent. Toutefois, l’application doit être conçue avec ce. Par exemple, l’application et les services qu’elle utilise doivent être sans état, qui autorise les requêtes être routé vers n’importe quelle instance. Cela empêche également l’ajout ou la suppression des instances spécifiques d’affecter de manière négative les utilisateurs actuels. Vous devez également implémenter la détection automatique d’instances ou de configuration qu’ils sont ajoutés et supprimés, afin que le code de l’application peut effectuer le routage nécessaire. Par exemple, une application web peut utiliser un ensemble de files d’attente dans une approche de la répétition alternée pour acheminer les demandes vers les services d’arrière-plan en cours d’exécution dans les rôles de travail. L’application web doit être en mesure de détecter des modifications dans le nombre de files d’attente, pour router les demandes et équilibrer la charge sur l’application.
- **Échelle en tant qu’unité**. Planifier les ressources supplémentaires pour s’adapter à la croissance. Pour chaque ressource, connaître les limites de mise à l’échelle supérieure et permet de dépasser ces limites ont ou décomposition. Déterminer les unités d’échelle pour le système en termes de jeux bien définis de ressources. Cela rend application horizontale plus facile et moins sujette à un impact négatif sur l’application par limitations imposées par manque de ressources dans une partie de l’ensemble du système. Par exemple, ajout x nombre de rôles web et worker peut nécessiter un nombre y de files d’attente supplémentaires et de numéro de plan de comptes de stockage pour gérer la charge de travail supplémentaire générée par les rôles. Pour une unité d’échelle peut consister à x rôles web et worker, files d’attente de _y_ et _z_ les comptes de stockage. Concevoir l’application de sorte qu’il est facilement mis à l’échelle en ajoutant une ou plusieurs unités de l’échelle.
- **Évitez d’affinité du client**. Si possible, assurez-vous que l’application ne requiert pas d’affinité. Les demandes peuvent ainsi être acheminés vers n’importe quelle instance, et le nombre d’instances est sans importance. Cela évite également la surcharge de stockage, d’extraction et de gestion des informations d’état pour chaque utilisateur.
- **Tirer parti des fonctionnalités de la plate-forme autoscaling**. Où la plateforme d’hébergement prend en charge une fonctionnalité autoscaling, telles qu’échelle d’Azure, préférez aux mécanismes personnalisés ou tiers, à moins que le mécanisme intégré ne peut pas répondre à vos besoins. Utiliser les règles de mise à l’échelle planifiées où pour s’assurer que les ressources sont disponibles sans délai de démarrage, mais ajoutez autoscaling réactive aux règles, le cas échéant faire face aux changements inattendus dans la demande. Vous pouvez utiliser les opérations autoscaling dans l’API de gestion de Service à ajuster autoscaling et pour ajouter des compteurs personnalisés aux règles. Pour plus d’informations, consultez le [Guide de mise à l’échelle](best-practices-auto-scaling.md).
- **Déchargement des tâches de processeur/e/s intensives en tant que tâches d’arrière-plan**. Si une demande à un service doit prendre beaucoup de temps pour exécuter ou absorber des ressources considérables, décharger le traitement de cette demande à une autre tâche. Utiliser des rôles de travail ou les tâches en arrière-plan (en fonction de la plateforme d’hébergement) pour exécuter ces tâches. Cette stratégie permet au service de continuer à recevoir des demandes et de rester réactive.  Pour plus d’informations, consultez le [Guide des travaux en arrière-plan](best-practices-background-jobs.md).
- **Répartir la charge de travail pour les tâches en arrière-plan**. Où il existe de nombreuses tâches d’arrière-plan ou les tâches requièrent beaucoup de temps ou de ressources, répartir le travail sur plusieurs unités de calcul (par exemple les rôles worker ou des travaux en arrière-plan). Pour une solution possible, consultez le [Modèle de consommateurs en concurrence](https://msdn.microsoft.com/library/dn568101.aspx).
- **Pensez que vers un _type shared-nothing_ architecture**. Une architecture de type shared-nothing utilise les nœuds indépendants et autonomes qui ne présentent aucun point unique de conflit (par exemple, de stockage ou des services partagés). En théorie, un tel système peut évoluer quasiment indéfiniment. Pendant une approche entièrement type shared-nothing n’est généralement pas pratique pour la plupart des applications, il peut fournir des opportunités de conception pour une meilleure évolutivité. Par exemple, évitant ainsi l’utilisation de l’état de session côté serveur, l’affinité du client et partitionnement de données sont de bons exemples de déplacement vers une architecture de type shared-nothing.

## <a name="data-management"></a>Gestion des données

- **Utilisez le partitionnement des données**. Diviser les données dans plusieurs bases de données et les serveurs de base de données ou de conception des services de l’application pour utiliser le stockage de données qui peut fournir ce partitionnement de façon transparente (et des exemples de la base de données élastique de base de données SQL Azure, stockage Azure Table). Cette approche peut aider à optimiser les performances et de permettre la mise à l’échelle plus facilement. Il existe différentes techniques, horizontal, vertical, le partitionnement et fonctionnel. Vous pouvez utiliser une combinaison de ces éléments pour en optimiser les avantages de performances des requêtes accrues, évolutivité plus simple, une gestion plus souple, une meilleure disponibilité et pour faire correspondre le type de magasin pour les données qu’il contiendra. Envisagez également d’utiliser les différents types de magasin de données pour différents types de données, en choisissant les types en fonction de la façon dont ils sont optimisés pour le type de données spécifique. Cela peut inclure à l’aide d’un magasin de données de colonne de la gamme à la place, ou bien comme stockage de table ou une base de données du document, comme une base de données relationnelle. Pour plus d’informations, consultez [conseils de partitionnement de données](best-practices-data-partitioning.md).
- **Conception de cohérence éventuelle**. La cohérence éventuelle améliore l’évolutivité en réduisant ou en supprimant le temps nécessaire pour synchroniser les données connexes partitionnées sur plusieurs magasins. Le coût est que données ne sont pas toujours cohérentes lorsqu’il est lu et certains écrire des opérations peuvent provoquer des conflits. La cohérence éventuelle est idéale pour les situations où les mêmes données lues fréquemment est écrites rarement. Pour plus d’informations, consultez l' [Introduction de la cohérence des données](https://msdn.microsoft.com/library/dn589800.aspx).
- **Réduire les interactions bavardages entre les composants et les services**. Éviter de concevoir des interactions dans lequel une application est requise pour effectuer plusieurs appels à un service (chacun d’eux retourne une petite quantité de données), au lieu d’un seul appel qui peut renvoyer toutes les données. Lorsque cela est possible, combinez plusieurs opérations connexes dans une seule demande lorsque l’appel est à un service ou un composant possédant une latence importante. Cela rend plus facile de surveiller les performances et d’optimiser les opérations complexes. Par exemple, utiliser des procédures stockées dans les bases de données pour encapsuler une logique complexe et de réduire le nombre d’aller-retour et le verrouillage des ressources.
- **Utiliser des files d’attente au niveau de la charge pour les écritures de données à haute vitesse**. Pics de la demande d’un service peuvent submerger de ce service et entraîner des échecs de remontée. Pour éviter cela, envisagez d’implémenter le [modèle de l’audit de charge basé sur la file d’attente](https://msdn.microsoft.com/library/dn589783.aspx). Utiliser une file d’attente qui agit comme un tampon entre une tâche et qu’il appelle un service. Cela peut lisser des charges qui peuvent entraîner l’échec du service ou la tâche délai d’attente.
- **Réduire la charge sur le magasin de données**. Le magasin de données est le plus souvent un goulot d’étranglement de traitement, une ressource coûteuse et souvent pas facile à faire évoluer. Si possible, supprimer le magasin de données logique (par exemple, le traitement des documents XML ou objets JSON) et effectuer le traitement de l’application. Par exemple, au lieu de passer XML vers la base de données (autre que sous la forme d’une chaîne opaque pour le stockage), sérialiser ou désérialiser le XML au sein de la couche application et les passer dans un formulaire qui est natif au magasin de données. Il est généralement plus facile de faire évoluer l’application que le magasin de données, donc vous devez tenter de faire autant de traitement intensives que possible au sein de l’application.
- **Réduire le volume de données récupérées**. Récupérer uniquement les données que vous avez besoin en spécifiant des colonnes et des critères pour sélectionner des lignes. Vérifiez l’utilisation de paramètres de valeur de table et le niveau d’isolement. Utiliser des mécanismes tels que des balises d’entité pour éviter l’extraction de données inutilement.
- **Utiliser activement la mise en cache**. Utiliser la mise en cache à chaque fois que possible afin de réduire la charge sur les ressources et services de générer ou de fournissent des données. La mise en cache est généralement adapté aux données relativement statique, ou qui requiert un traitement considérable à obtenir. La mise en cache doit se produire à tous les niveaux le cas échéant dans chaque couche de l’application, y compris la generation d’interface utilisateur et accès aux données. Pour plus d’informations, consultez le [Guide de mise en cache](best-practices-caching.md).
- **Gérer la croissance des données et rétention**. Il augmente la quantité de données stockées par une application dans le temps. Cette croissance augmente les coûts de stockage et augmente le temps de latence lors de l’accès aux données, qui affecte les performances et le débit de l’application. Il peut être possible d’archivage des données anciennes qui n’est plus accessible, ou déplacer les données rarement utilisées dans le stockage à long terme qui est plus économique, même si la latence d’accès est plus élevée.
- **Optimiser les transférer des objets de données (DTO) à l’aide d’un format binaire efficace**. Les DTO sont passées entre les couches d’une application plusieurs fois. En réduisant la taille permet de réduire la charge sur les ressources et le réseau. Toutefois, équilibrer les économies réalisées avec la charge de la conversion des données dans le format requis à chaque emplacement où il est utilisé. Adopter un format qui a l’interopérabilité maximale pour permettre la réutilisation facile d’un composant.
- **Définir le contrôle de cache**. Concevoir et configurer l’application pour utiliser la mise en cache de sortie ou de fragmenter la mise en cache lorsque cela est possible, afin de réduire la charge de traitement.
- **Activer la mise en cache du côté client**. Les applications Web doivent activer les paramètres de cache du contenu pouvant être mis en cache. Cela est généralement désactivée par défaut. Configurer le serveur pour remettre le cache approprié des en-têtes de contrôle pour activer la mise en cache de contenu sur les clients et les serveurs proxy.
- **Stockage des objets blob Azure d’utilisation et le réseau de diffusion de contenu Azure pour réduire la charge sur l’application**. Envisagez de stocker un contenu statique ou relativement statique public, tels que des images, des ressources, des scripts et des feuilles de style, dans le stockage blob. Cette approche évite l’application de la charge provoquée par la génération dynamique de ce contenu pour chaque demande. En outre, envisagez d’utiliser le réseau de livraison de contenu à mettre en cache ce contenu et le remettre aux clients. L’utilisation du réseau de livraison de contenu peut améliorer les performances au niveau du client, car le contenu est fourni à partir du centre de données géographiquement plus proche qui contient un cache de réseau de distribution de contenu. Pour plus d’informations, consultez les [Conseils de réseau de distribution contenu](best-practices-cdn.md).
- **Optimisation et réglage des requêtes SQL et les index**. Certains les instructions T-SQL ou des constructions peuvent avoir un impact sur les performances qui peuvent être réduits grâce à l’optimisation du code dans une procédure stockée. Par exemple, évitez la conversion des types **datetime** de **varchar** avant de le comparer avec une valeur littérale de **datetime** . Utilisez à la place des fonctions de comparaison de date/heure. Absence d’index appropriés peut également ralentir l’exécution de la requête. Si vous utilisez une structure de mappage objet/relationnel, comprendre comment il fonctionne et comment il peut affecter les performances de la couche d’accès aux données. Pour plus d’informations, reportez-vous à la section [Réglage de la requête](https://technet.microsoft.com/library/ms176005.aspx).
- **Pensez à la normalisation des données**. Normalisation des données permet d’éviter les doublons et incohérences. Cependant, maintenir plusieurs index, vérification de l’intégrité référentielle, effectuant plusieurs accès à petits segments de données et la jointure de tables pour réassembler les données impose une surcharge qui peut affecter les performances. Prendre en compte si un volume de stockage supplémentaires et de duplication est acceptable afin de réduire la charge sur le magasin de données. Déterminez également si l’application elle-même (qui est généralement plus facile à mettre à l’échelle) peut être invoquée de prendre en charge les tâches telles que la gestion de l’intégrité référentielle afin de réduire la charge sur le magasin de données. Pour plus d’informations, consultez [conseils de partitionnement de données](https://github.com/mspnp/azure-guidance/blob/master/Data%20partitioning.md).

## <a name="service-implementation"></a>Mise en œuvre du service
- **Utiliser des appels asynchrones**. Utiliser code asynchrone dans la mesure du possible lors de l’accès aux ressources ou aux services qui peuvent être limités par les e/s ou la bande passante du réseau ou qui ont une latence importante, afin d’éviter le verrouillage du thread appelant. Pour implémenter des opérations asynchrones, utiliser le [Modèle asynchrone basé sur des tâches (TAP)](https://msdn.microsoft.com/library/hh873175.aspx).
- **Éviter le verrouillage des ressources et d’utiliser une approche optimiste à la place**. Jamais verrouille l’accès aux ressources de stockage ou autres services qui ont une latence importante, car il s’agit d’une cause principale des problèmes de performances. Utilisez toujours optimistes approches de la gestion des opérations simultanées, telles que l’écriture dans le stockage. Utiliser les fonctionnalités de la couche de stockage pour gérer les conflits. Dans les applications distribuées, données peuvent être uniquement finalement cohérentes.
- **Compresser des données fortement compressibles sur une latence élevée, réseaux à bande passante faible**. Dans la majorité des cas dans une application web, le plus grand volume de données générées par l’application et transmises sur le réseau est des réponses aux demandes du client HTTP. La compression HTTP permet de réduire ce considérablement, en particulier pour le contenu statique. Cela peut réduire le coût, ainsi que la réduction de la charge sur le réseau, bien que la compression de contenu dynamique s’applique-t-elle une charge fractionnant plus élevée sur le serveur. Dans les autres plus généralisée des environnements, la compression des données peut réduire le volume de données transmises et réduire les coûts et les temps de transfert, mais les processus de compression et de décompression provoquer une surcharge. En tant que tel, compression seulement doit être utilisée que lorsqu’il y a un gain démontrable de performances. Autres méthodes de sérialisation, JSON ou des codages binaires, peuvent réduire la taille de charge utile tout en ayant un impact moindre sur les performances, alors que XML est susceptible d’augmenter.
- **Réduire le temps que les ressources et les connexions sont en cours d’utilisation**. Tenir à jour les connexions et les ressources que pour tant que vous devez les utiliser. Par exemple, ouvrir les connexions aussi tard que possible et leur permettre d’être retournée au pool de connexion dès que possible. Acquérir les ressources aussi tard que possible et de les supprimer dès que possible.
- **Réduire le nombre de connexions requis**. Connexions au service absorbent des ressources. Limiter le nombre requis et vous assurer que les connexions existantes sont réutilisées lorsque cela est possible. Par exemple, après l’exécution de l’authentification, utiliser l’emprunt d’identité le cas échéant, d’exécuter du code avec une identité spécifique. Cela peut vous aider à optimiser l’utilisation du pool de connexions en réutilisant des connexions.

    > [AZURE.NOTE]: APIs for some services automatically reuse connections, provided service-specific guidelines are followed. It's important that you understand the conditions that enable connection reuse for each service that your application uses.

- **Envoyer des requêtes par lots afin d’optimiser l’utilisation du réseau**. Par exemple, envoyer et lire des messages dans des lots lors de l’accès à une file d’attente et effectuer plusieurs lectures ou écritures sous forme de lot lors de l’accès au stockage ou un cache. Cela peut vous aider à optimiser l’efficacité des banques de données et des services en réduisant le nombre d’appels sur le réseau.
- **Évitez d’une exigence pour stocker l’état de session côté serveur** lorsque cela est possible. Gestion d’état de session côté serveur nécessite généralement l’affinité du client (qui est, de routage chaque demande à la même instance de serveur), qui affecte la capacité du système à l’échelle. Dans l’idéal, vous devez concevoir des clients sans état en ce qui concerne les serveurs qu’ils utilisent. Toutefois, si l’application doit mettre à jour l’état de session, stocker des données sensibles ou grands volumes de données de chaque client dans un cache côté serveur distribué qui peuvent accéder à toutes les instances de l’application.
- **Optimiser les schémas de stockage de table**. Lors de l’utilisation de magasins de table qui requièrent des noms de la table et de colonne doivent être transmises et traitées avec chaque requête, comme le stockage par table Azure, envisagez d’utiliser des noms plus courts pour réduire cette surcharge. Toutefois, veillez à préserver la lisibilité ou la facilité de gestion en utilisant des noms plus compact.
- **Utilisez la bibliothèque TPL (Task Parallel) pour exécuter des opérations asynchrones**. La bibliothèque parallèle de tâches rend facile d’écrire le code qui effectue des opérations d’e/S asynchrone. Utilisez _ConfigureAwait (false)_ lorsque cela est possible afin d’éliminer la dépendance d’une continuation sur un contexte de synchronisation. Cela réduit les risques de blocage de la thread.
- **Créer des dépendances de ressources pendant le déploiement ou au démarrage de l’application**. Évitez les appels répétés à des méthodes tester l’existence d’une ressource et ensuite créer la ressource s’il n’existe pas. (Les méthodes telles que _CloudTable.CreateIfNotExists_ et _CloudQueue.CreateIfNotExists_ dans la bibliothèque cliente du stockage Azure suivent ce modèle). Ces méthodes peuvent imposer des frais considérables si elles sont appelées avant chaque accès à une table de stockage ou de la file d’attente de stockage. À la place :
 - Créer les ressources requises lorsque l’application est déployée, ou lors du premier démarrage (un seul appel à _CreateIfNotExists_ pour chaque ressource dans le code de démarrage pour un rôle web ou travailleur est acceptable). Toutefois, veillez à gérer les exceptions qui peuvent survenir si votre code essaie d’accéder à une ressource qui n’existe pas. Dans ces situations, vous devez enregistrer l’exception et de prévenir un opérateur qu’une ressource est manquante.
 - Dans certaines circonstances, il peut être approprié de créer la ressource manquante dans le cadre de la gestion du code des exceptions. Mais vous devez procéder avec prudence comme la non existence de la ressource peut être le signe d’une erreur de programmation (un nom de ressource mal orthographié par exemple), ou un autre problème au niveau de l’infrastructure.
- **Structures d’utilisation légères**. Choisissez soigneusement l’API et des infrastructures qui que vous permet de réduire la charge globale sur l’application, moment de l’exécution et l’utilisation des ressources. Par exemple, à l’aide des API Web pour gérer les demandes de service peut réduire l’encombrement de l’application et augmenter la vitesse d’exécution, mais il peut ne pas convenir pour des scénarios avancés où les fonctionnalités supplémentaires de Windows Communication Foundation sont requises.
- **Envisagez de réduire le nombre de comptes de service**. Par exemple, utiliser un compte spécifique pour accéder aux ressources ou services qui imposent un nombre maximal de connexions, ou d’effectuent une meilleure où sont conservées les moins de connexions. Cette approche est commune pour les services de bases de données, mais il peut affecter la capacité à auditer avec précision les opérations en raison de l’emprunt d’identité de l’utilisateur d’origine.
- **Effectuer des tests de charge et de profilage des performances** au cours du développement, dans le cadre de routines de test et avant la version finale afin de garantir que l’application exécute et échelles selon les besoins. Ce test doit se produire sur le même type de matériel comme la plate-forme de production et avec les mêmes types et quantités de données et d’utilisateur se chargement qu’il rencontrera dans la production. Pour plus d’informations, reportez-vous à la section [tester les performances d’un service cloud](vs-azure-tools-performance-profiling-cloud-services.md).
